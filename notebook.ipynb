{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/belatijagadbintangsyuhada/.pyenv/versions/3.11.9/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import ssl # Quickfix to torchaudio ssl error\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "    image = cv2.resize(image, (224, 224))  # Resize to 224x224\n",
    "    image = image / 255.0  # Normalize to [0, 1]\n",
    "    image = np.transpose(image, (2, 0, 1))  # Convert to (C, H, W)\n",
    "    image = torch.tensor(image, dtype=torch.float32)\n",
    "    return image\n",
    "\n",
    "def show_image(dataloader, index):\n",
    "    # Get a batch of data\n",
    "    data_iter = iter(dataloader)\n",
    "    left_images, right_images, labels = next(data_iter)\n",
    "\n",
    "    # Convert tensor to image format\n",
    "    left_image = left_images[index]\n",
    "    right_image = right_images[index]\n",
    "\n",
    "    # Convert tensor to numpy array and denormalize if needed\n",
    "    left_image_np = left_image.numpy().transpose((0, 1, 2))\n",
    "    right_image_np = right_image.numpy().transpose((0, 1, 2))\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Left Foot\")\n",
    "    plt.imshow(left_image_np)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Right Foot\")\n",
    "    plt.imshow(right_image_np)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiabeticDataset(Dataset):\n",
    "    def __init__(self, root_dir: str, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        \n",
    "        for label in os.listdir(root_dir):\n",
    "            label_dir = os.path.join(root_dir, label)\n",
    "            if not os.path.isdir(label_dir):\n",
    "                continue\n",
    "            for img_name in os.listdir(label_dir):\n",
    "                if 'L' in img_name:\n",
    "                    left_img_path = os.path.join(label_dir, img_name)\n",
    "                    right_img_name = img_name.replace('L', 'R')\n",
    "                    right_img_path = os.path.join(label_dir, right_img_name)\n",
    "                    if os.path.exists(right_img_path):\n",
    "                        self.data.append((left_img_path, right_img_path, label))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        left_img_path, right_img_path, label = self.data[idx]\n",
    "        left_image = cv2.imread(left_img_path)\n",
    "        right_image = cv2.imread(right_img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            left_image = self.transform(left_image)\n",
    "            right_image = self.transform(right_image)\n",
    "        \n",
    "        label = 1 if label == 'diabetic' else 0\n",
    "        \n",
    "        return left_image, right_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = './images/train'\n",
    "val_dir = './images/val'\n",
    "\n",
    "train_dataset = DiabeticDataset(train_dir, transform=preprocessing)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = DiabeticDataset(val_dir, transform=preprocessing)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        # self.dropout = nn.Dropout2d(p=.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        # x = self.dropout(x)\n",
    "        x = F.gelu(x)\n",
    "        return F.max_pool2d(x, 2)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv_block_left = nn.Sequential(\n",
    "            ConvBlock(3, 32),\n",
    "            ConvBlock(32, 64),\n",
    "            ConvBlock(64, 128),\n",
    "            ConvBlock(128, 256),\n",
    "            ConvBlock(256, 512),\n",
    "        )\n",
    "        self.conv_block_right = nn.Sequential(\n",
    "            ConvBlock(3, 32),\n",
    "            ConvBlock(32, 64),\n",
    "            ConvBlock(64, 128),\n",
    "            ConvBlock(128, 256),\n",
    "            ConvBlock(256, 512),\n",
    "        )\n",
    "        self.fc1 = nn.Linear(512 * 7 * 7 * 2, 512)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, left_image, right_image):\n",
    "        x_left = self.conv_block_left(left_image)\n",
    "        x_right = self.conv_block_right(right_image)\n",
    "        \n",
    "        x_left = x_left.reshape(x_left.size(0), -1)\n",
    "        x_right = x_right.reshape(x_right.size(0), -1)\n",
    "        \n",
    "        x = torch.cat((x_left, x_right), dim=1)\n",
    "        \n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv_block_left = nn.Sequential(\n",
    "            ConvBlock(3, 32),\n",
    "            ConvBlock(32, 64),\n",
    "            ConvBlock(64, 128),\n",
    "            ConvBlock(128, 256),\n",
    "            ConvBlock(256, 512),\n",
    "        )\n",
    "        self.conv_block_right = nn.Sequential(\n",
    "            ConvBlock(3, 32),\n",
    "            ConvBlock(32, 64),\n",
    "            ConvBlock(64, 128),\n",
    "            ConvBlock(128, 256),\n",
    "            ConvBlock(256, 512),\n",
    "        )\n",
    "        self.fc1 = nn.Linear(512 * 2, 512)  # 512 from each branch after GAP\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, left_image, right_image):\n",
    "        # Extract features from both images\n",
    "        x_left = self.conv_block_left(left_image)\n",
    "        x_right = self.conv_block_right(right_image)\n",
    "        \n",
    "        # Apply global average pooling\n",
    "        x_left = F.adaptive_avg_pool2d(x_left, 1).reshape(x_left.size(0), -1)\n",
    "        x_right = F.adaptive_avg_pool2d(x_right, 1).reshape(x_right.size(0), -1)\n",
    "        \n",
    "        # Concatenate the features from both branches\n",
    "        x = torch.cat((x_left, x_right), dim=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV3Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MobileNetV3Model, self).__init__()\n",
    "        mobilenet = models.mobilenet_v3_large(pretrained=True)\n",
    "        \n",
    "        self.feature_extractor = mobilenet.features\n",
    "        \n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = True\n",
    "        \n",
    "        self.out_features = mobilenet.classifier[0].in_features\n",
    "\n",
    "        self.fc1 = nn.Linear(self.out_features * 2, 512)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, left_image, right_image):\n",
    "        x_left = self.feature_extractor(left_image)\n",
    "        x_right = self.feature_extractor(right_image)\n",
    "        \n",
    "        x_left = F.adaptive_avg_pool2d(x_left, 1).reshape(x_left.size(0), -1)\n",
    "        x_right = F.adaptive_avg_pool2d(x_right, 1).reshape(x_right.size(0), -1)\n",
    "        \n",
    "        x = torch.cat((x_left, x_right), dim=1)\n",
    "        \n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetModel, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        \n",
    "        self.out_features = resnet.fc.in_features\n",
    "\n",
    "        self.fc1 = nn.Linear(self.out_features * 2, 512)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, left_image, right_image):\n",
    "        x_left = self.feature_extractor(left_image)\n",
    "        x_right = self.feature_extractor(right_image)\n",
    "        \n",
    "        x_left = F.adaptive_avg_pool2d(x_left, 1).reshape(x_left.size(0), -1)\n",
    "        x_right = F.adaptive_avg_pool2d(x_right, 1).reshape(x_right.size(0), -1)\n",
    "        \n",
    "        x = torch.cat((x_left, x_right), dim=1)\n",
    "        \n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, epochs, optimizer, loss_fn, data):\n",
    "    for t in range(epochs):\n",
    "        loop = tqdm(data, total=len(data))\n",
    "        model.train()\n",
    "\n",
    "        for _, (left, right, y) in enumerate(loop):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            left, right, y = left.to(device), right.to(device), y.to(device)\n",
    "            \n",
    "            y = y.unsqueeze(1).float() \n",
    "            pred = model(left, right)\n",
    "            loss = loss_fn(pred, y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loop.set_description(f\"Epoch [{t+1}/{epochs}]\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "def validation_loop(model, loss_fn, data_loader, device):\n",
    "    model.eval()\n",
    "    size = len(data_loader.dataset)\n",
    "    num_batches = len(data_loader)\n",
    "    test_loss, correct = 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for left, right, y in data_loader:\n",
    "            left, right, y = left.to(device), right.to(device), y.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            pred = model(left, right)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_fn(pred, y.unsqueeze(1).float())\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate number of correct predictions\n",
    "            pred_labels = (pred > 0.5).float()  # Thresholding for binary classification\n",
    "            correct += (pred_labels.squeeze() == y).sum().item()\n",
    "        \n",
    "    # Average loss and accuracy\n",
    "    test_loss /= num_batches\n",
    "    accuracy = (correct / size) * 100\n",
    "\n",
    "    print(f\"Validation Error: \\n Accuracy: {accuracy:>0.1f}%, Avg loss: {test_loss:>8f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/35]: 100%|██████████| 4/4 [00:03<00:00,  1.02it/s, loss=0.422]\n",
      "Epoch [2/35]: 100%|██████████| 4/4 [00:01<00:00,  2.83it/s, loss=0.438]\n",
      "Epoch [3/35]: 100%|██████████| 4/4 [00:01<00:00,  2.55it/s, loss=0.397]\n",
      "Epoch [4/35]: 100%|██████████| 4/4 [00:02<00:00,  1.70it/s, loss=0.345]\n",
      "Epoch [5/35]: 100%|██████████| 4/4 [00:02<00:00,  1.95it/s, loss=0.423]\n",
      "Epoch [6/35]: 100%|██████████| 4/4 [00:01<00:00,  3.00it/s, loss=0.388]\n",
      "Epoch [7/35]: 100%|██████████| 4/4 [00:02<00:00,  1.88it/s, loss=0.374]\n",
      "Epoch [8/35]: 100%|██████████| 4/4 [00:01<00:00,  2.85it/s, loss=0.322]\n",
      "Epoch [9/35]: 100%|██████████| 4/4 [00:01<00:00,  2.61it/s, loss=0.353]\n",
      "Epoch [10/35]: 100%|██████████| 4/4 [00:01<00:00,  2.96it/s, loss=0.348]\n",
      "Epoch [11/35]: 100%|██████████| 4/4 [00:01<00:00,  3.17it/s, loss=0.358]\n",
      "Epoch [12/35]: 100%|██████████| 4/4 [00:01<00:00,  3.23it/s, loss=0.524]\n",
      "Epoch [13/35]: 100%|██████████| 4/4 [00:02<00:00,  1.75it/s, loss=0.323]\n",
      "Epoch [14/35]: 100%|██████████| 4/4 [00:01<00:00,  2.94it/s, loss=0.463]\n",
      "Epoch [15/35]: 100%|██████████| 4/4 [00:01<00:00,  2.86it/s, loss=0.4]  \n",
      "Epoch [16/35]: 100%|██████████| 4/4 [00:01<00:00,  3.10it/s, loss=0.315]\n",
      "Epoch [17/35]: 100%|██████████| 4/4 [00:01<00:00,  3.03it/s, loss=0.359]\n",
      "Epoch [18/35]: 100%|██████████| 4/4 [00:01<00:00,  3.04it/s, loss=0.279]\n",
      "Epoch [19/35]: 100%|██████████| 4/4 [00:01<00:00,  3.07it/s, loss=0.298]\n",
      "Epoch [20/35]: 100%|██████████| 4/4 [00:01<00:00,  3.04it/s, loss=0.255]\n",
      "Epoch [21/35]: 100%|██████████| 4/4 [00:01<00:00,  2.90it/s, loss=0.515]\n",
      "Epoch [22/35]: 100%|██████████| 4/4 [00:01<00:00,  2.26it/s, loss=0.297]\n",
      "Epoch [23/35]: 100%|██████████| 4/4 [00:01<00:00,  2.78it/s, loss=0.389]\n",
      "Epoch [24/35]: 100%|██████████| 4/4 [00:01<00:00,  3.04it/s, loss=0.395]\n",
      "Epoch [25/35]: 100%|██████████| 4/4 [00:01<00:00,  3.02it/s, loss=0.251]\n",
      "Epoch [26/35]: 100%|██████████| 4/4 [00:01<00:00,  3.07it/s, loss=0.321]\n",
      "Epoch [27/35]: 100%|██████████| 4/4 [00:01<00:00,  2.91it/s, loss=0.244]\n",
      "Epoch [28/35]: 100%|██████████| 4/4 [00:01<00:00,  2.94it/s, loss=0.34] \n",
      "Epoch [29/35]: 100%|██████████| 4/4 [00:01<00:00,  3.01it/s, loss=0.366]\n",
      "Epoch [30/35]: 100%|██████████| 4/4 [00:01<00:00,  3.03it/s, loss=0.357]\n",
      "Epoch [31/35]: 100%|██████████| 4/4 [00:01<00:00,  2.02it/s, loss=0.356]\n",
      "Epoch [32/35]: 100%|██████████| 4/4 [00:03<00:00,  1.14it/s, loss=0.329]\n",
      "Epoch [33/35]: 100%|██████████| 4/4 [00:02<00:00,  1.60it/s, loss=0.258]\n",
      "Epoch [34/35]: 100%|██████████| 4/4 [00:01<00:00,  2.54it/s, loss=0.368]\n",
      "Epoch [35/35]: 100%|██████████| 4/4 [00:02<00:00,  1.54it/s, loss=0.411]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n",
      "Validation Error: \n",
      " Accuracy: 87.2%, Avg loss: 0.248198\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = Model().to(device)\n",
    "\n",
    "epochs = 35\n",
    "optimizer = AdamW(params=model.parameters())\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "training_loop(model, epochs, optimizer, loss_fn, train_dataloader)\n",
    "\n",
    "validation_loop(model, loss_fn, val_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3666433"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
